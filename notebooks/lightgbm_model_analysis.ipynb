{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29120527",
   "metadata": {},
   "source": [
    "# Import Required Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ba7bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c693ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce7b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LightGBMBaselineClassifier import LightGBMBaselineClassifier\n",
    "from LightGBMTuner import LightGBMTuner\n",
    "from DiskIO import DiskIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d2ffb5",
   "metadata": {},
   "source": [
    "# Run Baseline LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e5096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = LightGBMBaselineClassifier()\n",
    "baseline.load_and_prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff89c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681547c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the evaluation results: (train_results, test_results)\n",
    "report, cm, stats = baseline.evaluate()\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"{k}: {v}%\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "baseline.plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b4805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'accuracy' because it's not a dict\n",
    "accuracy = report.pop('accuracy', None)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_report = pd.DataFrame(report).T  # Transpose to have labels as rows\n",
    "\n",
    "# Round float values\n",
    "df_report = df_report.round(3)\n",
    "\n",
    "# Add accuracy if present\n",
    "if accuracy is not None:\n",
    "    df_report.loc['accuracy'] = [None] * (df_report.shape[1] - 1) + [round(accuracy, 3)]\n",
    "\n",
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'baseline'\n",
    "baseline.save_model(pipeline=baseline.pipeline, label_encoder=baseline.label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86d99da",
   "metadata": {},
   "source": [
    "# Tuning - Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a03208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and tune\n",
    "tuner = LightGBMTuner(n_trials=30)\n",
    "tuner.tune_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33031b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best parameters\n",
    "print(\"Best Parameters Found:\")\n",
    "print(tuner.best_params)\n",
    "\n",
    "# Train final model\n",
    "tuner.train_final_model()\n",
    "\n",
    "# Plot learning curve\n",
    "tuner.plot_learning_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3bb1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on full Hao datasets and save final model\n",
    "tuner.train_and_save_final_model_on_full_data(suffix='final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e5560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on external data (Kotliarov dataset)\n",
    "report, cm, stats = tuner.evaluate_on_external_testset(\"../data/kotliarov.csv\", suffix='final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8709f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "\n",
    "# Remove 'accuracy' because it's not a dict\n",
    "accuracy = report.pop('accuracy', None)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_report = pd.DataFrame(report).T  # Transpose to have labels as rows\n",
    "\n",
    "# Round float values\n",
    "df_report = df_report.round(3)\n",
    "\n",
    "# Add accuracy if present\n",
    "if accuracy is not None:\n",
    "    df_report.loc['accuracy'] = [None] * (df_report.shape[1] - 1) + [round(accuracy, 3)]\n",
    "\n",
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666fa9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stats\n",
    "print(\"Tuned Model - Classification Report:\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"{k}: {v}%\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "tuner.plot_confusion_matrix(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbeee9b",
   "metadata": {},
   "source": [
    "# Baseline vs Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a00ed57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set to get feature names in correct order\n",
    "hao = pd.read_csv(\"../data/hao.csv\", index_col=0)\n",
    "expected_features = hao.drop(columns=[\"label\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28d7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Kotliarov test data (same format as hao.csv)\n",
    "kotliarov = pd.read_csv(\"../data/kotliarov.csv\", index_col=0)\n",
    "X_kot = kotliarov.drop(columns=[\"label\"])\n",
    "y_kot = kotliarov[\"label\"]\n",
    "\n",
    "# Align Kotliarov features to match training features\n",
    "X_kot_aligned = X_kot[expected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f1962",
   "metadata": {},
   "outputs": [],
   "source": [
    "io = DiskIO(\"../models\")\n",
    "\n",
    "baseline_model = io.load(\"LightGBM\")\n",
    "tuned_model = io.load(\"LightGBM\", \"final\")\n",
    "label_encoder = io.load(\"label_encoder_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d50a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of class labels the encoder was originally trained on\n",
    "known_labels = label_encoder.classes_\n",
    "\n",
    "# Create a boolean mask to keep only samples with labels the encoder recognizes\n",
    "# This is necessary to avoid transforming labels that were not seen during training (would raise an error)\n",
    "valid_mask = y_kot.isin(known_labels)\n",
    "\n",
    "# Align test features with training feature order and keep only valid rows\n",
    "# Note: 'expected_features' must match the training set columns exactly\n",
    "X_kot_aligned = X_kot[valid_mask][expected_features]\n",
    "\n",
    "# Filter the test labels to include only valid rows (i.e., rows with known labels)\n",
    "y_kot_filtered = y_kot[valid_mask]\n",
    "\n",
    "# Transform the filtered string labels into numerical form using the trained encoder\n",
    "y_kot_encoded = label_encoder.transform(y_kot_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ebf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetricsCore import MetricsCalculator\n",
    "\n",
    "metrics_calculator = MetricsCalculator()\n",
    "\n",
    "baseline_metrics = metrics_calculator.compute_from_model(baseline_model, X_kot_aligned, y_kot_encoded)\n",
    "tuned_metrics = metrics_calculator.compute_from_model(tuned_model, X_kot_aligned, y_kot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9743b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Baseline': baseline_metrics,\n",
    "    'Tuned': tuned_metrics\n",
    "})\n",
    "\n",
    "df.T.plot(kind='bar', figsize=(10, 6), rot=0)\n",
    "plt.ylabel(\"Metric Score (%)\")\n",
    "plt.title(\"Model Performance on Kotliarov Dataset - LightGBM\")\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915d8219",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list(baseline_metrics.keys())\n",
    "baseline_values = [baseline_metrics[m] for m in metrics]\n",
    "tuned_values = [tuned_metrics[m] for m in metrics]\n",
    "diff = [t - b for t, b in zip(tuned_values, baseline_values)]\n",
    "\n",
    "# Extract the list of metric names from the baseline results (e.g., Accuracy, F1, etc.)\n",
    "metrics = list(baseline_metrics.keys())\n",
    "\n",
    "# Collect metric values for baseline and tuned models in the same order\n",
    "baseline_values = [baseline_metrics[m] for m in metrics]\n",
    "tuned_values = [tuned_metrics[m] for m in metrics]\n",
    "\n",
    "# Compute the difference in performance for each metric (Tuned - Baseline)\n",
    "# This helps identify which metrics improved and by how much\n",
    "diff = [t - b for t, b in zip(tuned_values, baseline_values)]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(metrics, diff, color='skyblue')\n",
    "plt.axhline(0, color='black', linestyle='--')\n",
    "plt.title(\"Tuned - Baseline Metric Differences on Kotliarov\")\n",
    "plt.ylabel(\"Difference in Score (%)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96283586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels on the Kotliarov test set using both baseline and tuned models\n",
    "baseline_preds = baseline_model.predict(X_kot_aligned)\n",
    "tuned_preds = tuned_model.predict(X_kot_aligned)\n",
    "\n",
    "# Calculate the percentage of samples for which both models made the same prediction\n",
    "agreement = np.mean(baseline_preds == tuned_preds)\n",
    "\n",
    "# Display model agreement in percentage format\n",
    "print(f\"The two models agree on {agreement * 100:.2f}% of predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e752e15a",
   "metadata": {},
   "source": [
    "# Interpretation - SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b7de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Prepare background data for SHAP explainer\n",
    "background_data = tuned_model.named_steps[\"scaler\"].transform(X_kot_aligned.sample(100, random_state=42))\n",
    "\n",
    "# Get the XGBoost model from the pipeline\n",
    "xgb_model = tuned_model.named_steps[\"model\"]\n",
    "\n",
    "# Create SHAP explainer for the tuned model\n",
    "explainer = shap.Explainer(xgb_model, background_data)\n",
    "\n",
    "# Compute SHAP values for the (scaled) test set\n",
    "X_kot_scaled = tuned_model.named_steps[\"scaler\"].transform(X_kot_aligned)\n",
    "shap_values = explainer(X_kot_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SHAP summary\n",
    "shap.summary_plot(shap_values, X_kot_aligned, feature_names=expected_features, max_display=20, plot_type=\"bar\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
